---
title: "DS301_HW5"
author: "Aashwin Lamsal"
date: "4/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 Set up

```{r}
# Suppose p = 1 (X)
# Y is binary (Y=0 or Y=1)
# X|Y=0 ~ N(2,var = 3)
# X|Y=1 ~ N(4,var = 3)
# since we're given variance, we take the square root of 3 for our standard deviation to use in rnorm
# Proportion of Y=0 is 0.8 and proportion of Y=1 is 0.2

## training data
set.seed(11)
x0 = rnorm(800,2,sd=sqrt(3))
x1 = rnorm(200,4,sd=sqrt(3))
X_train = c(x0,x1)
Y_train = rep(c(0,1),c(800,200))

data.train = as.data.frame(cbind(Y_train,X_train))
head(data.train)

## testing data
set.seed(12)
x0_test = rnorm(800,2,sd=sqrt(3))
x1_test = rnorm(200,4,sd=sqrt(3))

X_test = c(x0_test,x1_test)
Y_test = rep(c(0,1),c(800,200))

data.test = as.data.frame(cbind(Y_test,X_test))
head(data.test)

### Bayes classifier 

#P(Y=0|X) = P(X|Y=0)P(Y=0)/P(X)
#P(Y=1|X) = P(X|Y=1)P(Y=1)/P(X)

prob_y0 = 0.8*dnorm(X_train,2,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)
prob_y1 = 0.2*dnorm(X_train,4,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)


Y = rep(1,1000)
for(i in 1:1000){
	if(prob_y0[i]>prob_y1[i]){
		Y[i] = 0
	}
}

table(Y,Y_test) #confusion matrix for the Bayes Classifier
mean(Y!=Y_test) #Bayes Error Rate, the lowest error rate that we can possibly expect
```
### The interpretation of the Bayes Classifier's decision boundary is written and shown in the word document.

## Question 1C
```{r}
library(MASS)
n = length(Y)
lda.fit = lda(Y_train~X_train,data=data.train)

lda.fit
names(lda.fit)

lda.fit$scaling
## variance of training is
1/lda.fit$scaling^2

# Estimating Pi hat values

count_0 = 0
count_1 = 0


for (i in 1:n){
  #finding the count of how many X's were assigned Y= 0 
  if((Y[i] == 0) == TRUE){
    count_0 = count_0 + 1
  }
  #finding the count of how many X's were assigned Y = 1
  if((Y[i] == 1) == TRUE){
    count_1 = count_1 + 1
  }
}


# Pi_hat is calculated as pi_hat = nk/n
pi.hat0 = count_0/n
pi.hat1 = count_1/n

pi.hat0
pi.hat1
```
### Variance estimate equaled 2.98, Pi0 hat estimate equaled 0.918 and Pi1 hat estimate equaled 0.082. Interpretation of why the estimates were not the values that we expected is in the word document.

## Question 1E
```{r}
#Using code from compare_classifiers.R
#Bayes classifier
## training data
set.seed(11)
x0 = rnorm(800,2,sd=sqrt(3))
x1 = rnorm(200,4,sd=sqrt(3))
X_train = c(x0,x1)
Y_train = rep(c(0,1),c(800,200))

data.train = as.data.frame(cbind(Y_train,X_train))
head(data.train)

## testing data
set.seed(12)
x0_test = rnorm(800,2,sd=sqrt(3))
x1_test = rnorm(200,4,sd=sqrt(3))

X_test = c(x0_test,x1_test)
Y_test = rep(c(0,1),c(800,200))

data.test = as.data.frame(cbind(Y_test,X_test))
head(data.test)

### Bayes classifier 

#P(Y=0|X) = P(X|Y=0)P(Y=0)/P(X)
#P(Y=1|X) = P(X|Y=1)P(Y=1)/P(X)

prob_y0 = 0.8*dnorm(X_train,2,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)
prob_y1 = 0.2*dnorm(X_train,4,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)


Y = rep(1,1000)
for(i in 1:1000){
	if(prob_y0[i]>prob_y1[i]){
		Y[i] = 0
	}
}

bayes_matrix = table(Y,Y_test) #confusion matrix for the Bayes Classifier
bayes_matrix
bayes_error_rate = mean(Y!=Y_test) #Bayes Error Rate, the lowest error rate that we can possibly expect
bayes_error_rate


#LDA Classifier
library(MASS)
lda.fit = lda(Y_train~X_train,data=data.train)

lda.fit
#names(lda.fit)

#lda.fit$scaling
## variance of training is
1/lda.fit$scaling^2

# formula to calculate variance here is: 
# 1/(n-K)*(n0*var(X_0) + n1*var(X_1))


lda.pred = predict(lda.fit,data.test)
#names(lda.pred)
#head(lda.pred$class) # automatically assigns Y to the class with largest probability 
#head(lda.pred$posterior)
#P(Y=0|X)
#P(Y=1|X)

t1 = table(lda.pred$class,Y_test)
lda_error_rate = mean(lda.pred$class!=Y_test)
t1
lda_error_rate

false_neg = t1[1,2]/sum(t1[,2]) #false_neg
false_pos = t1[2,1]/sum(t1[,1]) #false_pos

false_neg
false_pos

lda.pred$posterior[1:10,]


lda.class = rep(0,1000)
lda.class[lda.pred$posterior[,2]>0.45] = 1
t2 = table(lda.class,Y_test)
t2
mean(lda.class!=Y_test)
t2[1,2]/sum(t2[,2]) #false_neg
t2[2,1]/sum(t2[,1]) #false_pos
```
### The Bayes Error Rate is equal to 0.176 and the LDA Test Error Rate is equal to 0.18.



## Question 1F
```{r}
library(class)
X_train = as.matrix(X_train)
X_test = as.matrix(X_test)
knn.pred = knn(X_train,X_test,Y_train,k=3) #Let K=3
table(knn.pred,Y_test)
mean(knn.pred!=Y_test) 
```
### The KNN Test Error Rate is equal to 0.223.

## Question 1G
```{r}
plot(density(x0), xlim=c(-5,10))
points(density(x1), col = 'red', type='l')

```
### The plots show the distribution of the x0 and x1 data that we generated.

## Question 2
```{r}
spam = read.csv('C:/Users/aashw/Documents/DS-301/spambase.data', header = FALSE)

#Exploring the percentage of emails are spam vs. not spam 

spam_status = rep("No",4601)
#spam$spam_status = spam_status
spam_status_numbers = spam$V58

yes_count = 0

# using a for loop to iterate through the last column and assigning "Yes" to observations with a value of 1 and "No", in a new column
for (i in 1:4601){
  if (spam_status_numbers[i] == 1){
    spam_status[i] = "Yes"
    yes_count = yes_count + 1
  } 
}
spam$spam_status = spam_status

spam_length = length(spam$V1)

no_count = 4601 - yes_count

percentage_spam = yes_count/spam_length
percentage_spam 

percentage_notspam = no_count/spam_length
percentage_notspam

# Standardize our predictors. 

standardized.spam = scale(spam[,-59])
# var(standardized.spam[,50]) line used to check if scale function worked properly

# Question 2B: let's split the observations into test and training
# preserve the proportion of Yes vs. No
set.seed(1)
yes.index = which(spam$spam_status=='Yes')
no.index = which(spam$spam_status=='No')


num_yes = round((spam_length/2)*percentage_spam)
num_no = round((spam_length/2)* percentage_notspam)


train.yes = sample(yes.index, num_yes)
train.no = sample(no.index, num_no)
train = c(train.yes,train.no) #contains index numbers of observations we want in the training set
train.X = standardized.spam[train,]
dim(train.X)
train.Y = spam$spam_status[train]
fit.trainY = spam$V58[train]

train.Y.factor = as.factor(train.Y)

test.X = standardized.spam[-train,]
dim(test.X)
test.Y = spam$spam_status[-train]
fit.testY = spam$V58[-train]

# Question 2C
knn.pred = knn(train.X,test.X, train.Y,k=8) #out of all the k values I played around with, k = 8 gave me the lowest misclassification rate (0.0191) of 1.91%.

table(knn.pred,test.Y)
mean(knn.pred!=test.Y) 

#Logistic Regression

#Creating new dataframe of training data
training_glm = as.data.frame(cbind(train.X, fit.trainY))
#fitting the training data onto the model
glm.train = glm(train.Y.factor~train.X,data = spam, family=binomial)
#Creating new dataframe of testing data
testing_glm = as.data.frame(cbind(test.X, fit.testY))


glm.probs = predict(glm.train,testing_glm,type='response')
head(glm.probs)

glm.pred = rep(0,2301)
glm.pred[glm.probs>0.5] = 1 
table(glm.pred,fit.testY)
mean(glm.pred==fit.testY)
mean(glm.pred!=fit.testY)

#LDA
library(MASS)
lda.fit.C = lda(spam$spam_status~spam$V56+spam$V57,data=spam)

lda.fit.C

lda.pred.C = predict(lda.fit.C,data.test)
tC = table(lda.pred.C$class,spam$spam_status)
lda_error_rate = mean(lda.pred.C$class!=spam$spam_status)
tC
lda_error_rate

```


