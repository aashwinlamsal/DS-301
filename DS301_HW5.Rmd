---
title: "DS301_HW5"
author: "Aashwin Lamsal"
date: "4/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 Set up

```{r}
# Suppose p = 1 (X)
# Y is binary (Y=0 or Y=1)
# X|Y=0 ~ N(2,var = 3)
# X|Y=1 ~ N(4,var = 3)
# since we're given variance, we take the square root of 3 for our standard deviation to use in rnorm
# Proportion of Y=0 is 0.8 and proportion of Y=1 is 0.2

## training data
set.seed(11)
x0 = rnorm(800,2,sd=sqrt(3))
x1 = rnorm(200,4,sd=sqrt(3))
X_train = c(x0,x1)
Y_train = rep(c(0,1),c(800,200))

data.train = as.data.frame(cbind(Y_train,X_train))
head(data.train)

## testing data
set.seed(12)
x0_test = rnorm(800,2,sd=sqrt(3))
x1_test = rnorm(200,4,sd=sqrt(3))

X_test = c(x0_test,x1_test)
Y_test = rep(c(0,1),c(800,200))

data.test = as.data.frame(cbind(Y_test,X_test))
head(data.test)

### Bayes classifier 

#P(Y=0|X) = P(X|Y=0)P(Y=0)/P(X)
#P(Y=1|X) = P(X|Y=1)P(Y=1)/P(X)

prob_y0 = 0.8*dnorm(X_train,2,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)
prob_y1 = 0.2*dnorm(X_train,4,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)


Y = rep(1,1000)
for(i in 1:1000){
	if(prob_y0[i]>prob_y1[i]){
		Y[i] = 0
	}
}

table(Y,Y_test) #confusion matrix for the Bayes Classifier
mean(Y!=Y_test) #Bayes Error Rate, the lowest error rate that we can possibly expect
```
### The interpretation of the Bayes Classifier's decision boundary is written and shown in the word document.

## Question 1C
```{r}
library(MASS)
n = length(Y)
lda.fit = lda(Y_train~X_train,data=data.train)

lda.fit
names(lda.fit)

lda.fit$scaling
## variance of training is
1/lda.fit$scaling^2

# Estimating Pi hat values

count_0 = 0
count_1 = 0


for (i in 1:n){
  #finding the count of how many X's were assigned Y= 0 
  if((Y[i] == 0) == TRUE){
    count_0 = count_0 + 1
  }
  #finding the count of how many X's were assigned Y = 1
  if((Y[i] == 1) == TRUE){
    count_1 = count_1 + 1
  }
}


# Pi_hat is calculated as pi_hat = nk/n
pi.hat0 = count_0/n
pi.hat1 = count_1/n

pi.hat0
pi.hat1
```
### Variance estimate equaled 2.98, Pi0 hat estimate equaled 0.918 and Pi1 hat estimate equaled 0.082. Interpretation of why the estimates were not the values that we expected is in the word document.

## Question 1E
```{r}
#Using code from compare_classifiers.R
#Bayes classifier
## training data
set.seed(11)
x0 = rnorm(800,2,sd=sqrt(3))
x1 = rnorm(200,4,sd=sqrt(3))
X_train = c(x0,x1)
Y_train = rep(c(0,1),c(800,200))

data.train = as.data.frame(cbind(Y_train,X_train))
head(data.train)

## testing data
set.seed(12)
x0_test = rnorm(800,2,sd=sqrt(3))
x1_test = rnorm(200,4,sd=sqrt(3))

X_test = c(x0_test,x1_test)
Y_test = rep(c(0,1),c(800,200))

data.test = as.data.frame(cbind(Y_test,X_test))
head(data.test)

### Bayes classifier 

#P(Y=0|X) = P(X|Y=0)P(Y=0)/P(X)
#P(Y=1|X) = P(X|Y=1)P(Y=1)/P(X)

prob_y0 = 0.8*dnorm(X_train,2,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)
prob_y1 = 0.2*dnorm(X_train,4,sd=sqrt(3))/(dnorm(X_train,2,sd=sqrt(3))*0.8+dnorm(X_train,4,sd=sqrt(3))*0.2)


Y = rep(1,1000)
for(i in 1:1000){
	if(prob_y0[i]>prob_y1[i]){
		Y[i] = 0
	}
}

bayes_matrix = table(Y,Y_test) #confusion matrix for the Bayes Classifier
bayes_matrix
bayes_error_rate = mean(Y!=Y_test) #Bayes Error Rate, the lowest error rate that we can possibly expect
bayes_error_rate


#LDA Classifier
library(MASS)
lda.fit = lda(Y_train~X_train,data=data.train)

lda.fit
#names(lda.fit)

#lda.fit$scaling
## variance of training is
1/lda.fit$scaling^2

# formula to calculate variance here is: 
# 1/(n-K)*(n0*var(X_0) + n1*var(X_1))


lda.pred = predict(lda.fit,data.test)
#names(lda.pred)
#head(lda.pred$class) # automatically assigns Y to the class with largest probability 
#head(lda.pred$posterior)
#P(Y=0|X)
#P(Y=1|X)

t1 = table(lda.pred$class,Y_test)
lda_error_rate = mean(lda.pred$class!=Y_test)
t1
lda_error_rate

false_neg = t1[1,2]/sum(t1[,2]) #false_neg
false_pos = t1[2,1]/sum(t1[,1]) #false_pos
```
### The Bayes Error Rate is equal to 0.176 and the LDA Test Error Rate is equal to 0.18.



## Question 1F
```{r}
library(class)
X_train = as.matrix(X_train)
X_test = as.matrix(X_test)
knn.pred = knn(X_train,X_test,Y_train,k=3) #Let K=3
table(knn.pred,Y_test)
mean(knn.pred!=Y_test) 
```
### The KNN Test Error Rate is equal to 0.223.

## Question 1G
```{r}
plot(density(x0), xlim=c(-5,10))
points(density(x1), col = 'red', type='l')

```
### The plots show the distribution of the x0 and x1 data that we generated.


