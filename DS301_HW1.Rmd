---
title: "DS301_HW1"
author: "Aashwin Lamsal"
date: "2/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW #1 Problem 3a
```{r}
library(jtools) #necessary to create a table for our summary output
#reading Toluca.txt and placing into a variable
toluca = read.table("Toluca.txt", header = T) #Header is true since we have a header in our dataset

#head(toluca) # used to check if the data was read in properly

#Fitting a simple linear regression model: x= lotSize, y= workHrs

x = toluca$lotSize
y = toluca$workHrs
fit = lm(y~x)
summary = summary(fit) #summary of the output

summ(fit) #used for jtools table output

```
From the summary output:

Beta-naught = 62.37
Beta_one = 3.57
se(Beta-naught) = 26.18
se(Beta-one) = 0.35
Beta-naught p-value = 0.0259 approximately 0.03
Beta-one p-value = 4.45e-10 approximately 0.00

## HW #1 Problem 3b

```{r}
#Drawing scatterplot of workHrs versus lotSize
#Reestablishing x and y variables, and creating the fitted simple linear regression model
x = toluca$lotSize
y = toluca$workHrs
fit = lm(y~x)
#summary(fit) was used to double check that I got the same results as 3a

#Scatterplot creation
plot(x,y, main = "Toluca Work Hours vs. Lot Size", pch = 16, xlab = "Lot Size", ylab = "Work Hours")
abline(fit) #adding least squares line

```

## HW #1 Problem 3c
```{r}
#Reestablishing x and y variables, and creating the fitted simple linear regression model
x = toluca$lotSize
y = toluca$workHrs
fit = lm(y~x)

#names(fit) used to see names of the different attributes in the model

firstFiveFittedValues = fit$fitted.values[1:5]
firstFiveResidualValues = fit$residuals[1:5]

cat("The first 5 fitted values are as follows: \n", firstFiveFittedValues)

cat("\n \n The first 5 residual values are as follows: \n", firstFiveResidualValues)


```
## HW #1 Problem 4a

```{r}
#Reestablishing x and y variables, and creating the fitted simple linear regression model
x = toluca$lotSize
y = toluca$workHrs
fit = lm(y~x)
#summary(fit) used to double check
n = 25
df = n-2 #n=25, so our degrees of freedom would be: n-2 = 23
#Calculation for the estimate of sigma squared

sigmaSquaredHat = sum(fit$residuals^2)/df #sigma squared hat is equal to the summation of residual errors squared divided by n-2, which in our case is the df
cat("Sigma squared hat = ", sigmaSquaredHat)

```

## HW #1 Problem 4b
```{r}
#Reestablishing x and y variables, and creating the fitted simple linear regression model
x = toluca$lotSize
y = toluca$workHrs
fit = lm(y~x)


#use confint() to compute confidence intervals

confint(fit, level = 0.9 ) #confint uses t-quantiles from the t-distribution, confint.default uses the normal distribution/quantiles

```
The intercept represents beta-naught. The way to interpret this confidence interval is that we are 90% confident that our beta-naught value will fall between 17.50 and 107.23 when beta-one is equal to 0.

## HW #1 Problem 4c
```{r}
#Reestablishing x and y variables, and creating the fitted simple linear regression model

# Our hypothesis test is as follows
# H0: beta-one = 0
# H1: beta-one > 0
# Significance level (alpha) = 0.05
x = toluca$lotSize
y = toluca$workHrs
fit = lm(y~x)
#summary(fit) 

n = 25
df = n - 2
alpha = 0.05

# From the summary fit, we know beta-one-hat = 3.57
beta1Hat = 3.57
# test statistic ts = beta-one-hat/se(beta-one-hat)
#0.347 is the standard deviation of beta-one-hat, found from the summary output of our fit, which I used to calculate the standard error of beta-one-hat
seB1Hat = 0.347/sqrt(n) #this equals 0.0694
# calculating test statistic: ts = beta-one-hat/standard error of beta-one-hat
ts = beta1Hat/seB1Hat

#use ts and df to calculate p-value
pVal = pt(ts, df, lower.tail = FALSE)

#compare p-Value to our significance level
decisionRule = pVal < alpha
decisionRule
```
Since our p-value is less than our significance level, we reject the null hypothesis, meaning that we have evidence that beta-one is greater than 0 at the significane level of 0.05.

## HW #1 Problem 5
```{r}
# We know that the true underlying population regression line is:
# Y_i = 2 + 3*x_i + epsilon_i
# epsilon follows a normal distribution N(0,2^2)

#beta-naught = 2
#beta-one = 3
#standard deviation = 2

x = seq(0, 10, length.out = 100)

#5a) Generate 100 observations Yi under this normal error model for the for the X values from above:
y = 2 + 3*x

#5b) Draw a scatterplot of x and y:
plot(x,y, main = "Y vs. X", pch = 16, xlab = "x-values", ylab = "y-values")


#5c) Design a simple simulation to show that βˆ1 is an unbiased estimator of β1
# Taking 1000 samples of size 100

output = c(1:1000)
set.seed(1)
for(i in output){
  simSample = sample(y, 100, replace=FALSE)
 
  simFit = lm(simSample~x)
  output = c(output, simFit$coefficients[2])
  
}

#remove the double counting
output = output[1001:2000]
#output was called to check the values, they were a lot lower than anticipated and I can't seem to get them to the range they should be at

#5d) Creating the histogram
hist(output)
abline(v=3)

```

## The results I got are as shown above, I know this is not the correct answer, as the results that my model predicted were too small compared to what I was supposed to get. I'm a little confused as to what I need to do, and I'll probably be coming to office hours to work through this problem later this week.






